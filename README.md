
<!-- README.md is generated from README.Rmd. Please edit that file -->

# torch-for-R-gibbons

<!-- badges: start -->
<!-- badges: end -->

# Summary

The torch-for-R-gibbons repository is designed to facilitate the use of
convolutional neural networks (CNNs) for automated detection tasks in
passive acoustic monitoring (PAM) data, specifically targeting gibbon
vocalizations. Leveraging the torch package in R, this project aims to
streamline the process of training and evaluating deep learning models
on spectrogram images derived from acoustic recordings.

Key components of the repository include:

Model Training Scripts: R scripts that define and train CNN
architectures (e.g., AlexNet, VGG16) on labeled spectrogram datasets.

Evaluation Tools: Functions to assess model performance using metrics
like accuracy and AUC (Area Under the Curve).

Data Handling: Procedures for loading, preprocessing, and organizing
spectrogram images for model input.

For detailed usage instructions and examples, refer to the gibbonNetR
documentation (<https://denajgibbon.github.io/gibbonNetR/>).

Current citation: Clink, Dena J., et al. “Automated detection of gibbon
calls from passive acoustic monitoring data using convolutional neural
networks in the” torch for R” ecosystem.” arXiv preprint
arXiv:2407.09976 (2024).
